{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Q-LEARNING IN A CONTINUOUS ENVIRONMENT\n",
    "\n",
    "EXTRACTED and quoted FROM:\n",
    "- https://eckronsoftware.wordpress.com/2018/12/26/q-learning-with-cartpole/\n",
    "- https://github.com/ehennis/ReinforcementLearning/blob/master/02-QLearning.ipynb\n",
    "\n",
    "CART POLE\n",
    "\"The actions are 0 to push the cart to the left and 1 to push the cart to the right.\n",
    "\n",
    "The continuous state space is an X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole. The X coordinate goes from -4.8 to +4.8, velocity is -Inf to +Inf, angle of the pole goes from -24 degrees to +24 degrees, tip velocity is -Inf to +Inf. With all of the possible combinations you can see why we can't create a Q table for each one.\n",
    "\n",
    "To \"solve\" this puzzle you have to have an average reward of > 195 over 100 consecutive episodes\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np #for array manipulation\n",
    "import gym # pull the cart pole environment from Open AI.\n",
    "#from gym import wrappers\n",
    "#import random\n",
    "#import time\n",
    "#import math\n",
    "#from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "gc.disable() #Disable automatic garbage collection."
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "traceback": [
      "Error: Session cannot generate requests",
      "at w.executeCodeCell (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2031190714/out/client/extension.js:52:301310)",
      "at w.execute (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2031190714/out/client/extension.js:52:300703)",
      "at w.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2031190714/out/client/extension.js:52:296367)",
      "at t.CellExecutionQueue.executeQueuedCells (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2031190714/out/client/extension.js:52:311160)",
      "at t.CellExecutionQueue.start (/root/.vscode-server/extensions/ms-toolsai.jupyter-2021.8.2031190714/out/client/extension.js:52:310700)"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Create the Environment\n",
    "envCartPole = gym.make('CartPole-v1')\n",
    "start = envCartPole.reset()\n",
    "print('Here is the starting tuple with the 4 continuous variables:')\n",
    "print(start)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TEST APPLICATION TO GET COMMON VARIABLES"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Explore the state space to get an idea of possible combinations. \n",
    "- Since we are trying to use the FULL environment, ensure that you are not just pushing and pulling in the center location.\n",
    "- Loop through 1,000 episodes to get sample space of variables in the state space. The idea for this code comes from Miquel Morales who created a compliment to the GT Reinforcement Learning class."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "observations = []\n",
    "for episode in range(1000): #1k episodes should give us enough data\n",
    "    observation = envCartPole.reset()\n",
    "    for step in range(100): #Do only 100 steps per episode unless we finish\n",
    "        observations.append(observation)\n",
    "        action = envCartPole.action_space.sample()\n",
    "        if episode < 25: #First 20 episodes only go left\n",
    "            action = 0\n",
    "        elif episode < 50: #Next 20 episodes only go right\n",
    "            action = 1\n",
    "        observation, reward, done, info = envCartPole.step(action)\n",
    "        if done: #Fell or hit the goal\n",
    "            break\n",
    "envCartPole.close()\n",
    "x_vals = np.array(observations)[:,0]\n",
    "xd_vals = np.array(observations)[:,1]\n",
    "a_vals = np.array(observations)[:,2]\n",
    "ad_vals = np.array(observations)[:,3]\n",
    "y = np.zeros_like(x_vals)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot each of these variables to see their range in our sample. Using the graph below you can see the range of the X coordinate to range from about -1 to 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Plot results \n",
    "plt.plot(x_vals, y + 0.10, '.', color=\"royalblue\") # X Coordinate\n",
    "plt.plot(xd_vals, y + 0.05, '.', color=\"orange\") # Cart Velocity\n",
    "plt.plot(a_vals, y - 0.05, '.', color=\"darkcyan\") # Pole Angle\n",
    "plt.plot(ad_vals, y - 0.10, '.', color=\"tomato\") # Pole tip velocity\n",
    "plt.xlabel(\"x \")\n",
    "plt.ylabel(\"y \")\n",
    "plt.title(\"Loop through 1,000 episodes to get sample space of variables in the state space.\")\n",
    "plt.grid()\n",
    "plt.ylim([-0.15, 0.15])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# #Plot results \n",
    "# %matplotlib inline\n",
    "# #Name x-axis, y-axis and whole graph\n",
    "# plt.xlabel(\"episodes\")\n",
    "# plt.ylabel(\"average rewards\")\n",
    "# plt.title(\"FROZEN LAKE: AVG reward per 1000 episodes\")\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"Using the graph above you can see the range of the X coordinate to range from about -1 to 1. In OpenAI's GitHub, it is stated that this number should range from -4.8 to 4.8. Why do you think we didn't get that? Hopefully, you see that with our random action generation that we never held the pole up long enough to reach the edge. You would have to have a fairly functional learner to keep the pole balanced long enough to reach the edge.\"\n",
    "\n",
    "Here are the actual thresholds from teh environment:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(envCartPole.env.observation_space.low)\n",
    "print(envCartPole.env.observation_space.high)\n",
    "\n",
    "x_thres = ((envCartPole.env.observation_space.low/2)[0],\n",
    "           (envCartPole.env.observation_space.high/2)[0])\n",
    "a_thres = ((envCartPole.env.observation_space.low/2)[2],\n",
    "           (envCartPole.env.observation_space.high/2)[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DISCRETIZATION\n",
    "\n",
    "Since we have the common variables from the above test application we can go ahead and start to box these values up into groups that we can make discrete. To do this we can use the numpy method linspace that will return evenly spaced numbers.\n",
    "\n",
    "##### Here I will do the grouping for the X coordinate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#X COORDINATE\n",
    "x1 = np.linspace(np.min(x_vals), np.max(x_vals),\n",
    "                 4, endpoint=False)[1:]\n",
    "y1 = np.zeros(len(x1)) + 0.05\n",
    "\n",
    "plt.ylim([-0.075, 0.075])\n",
    "plt.plot(x1, y1, 'o', color=\"royalblue\")\n",
    "plt.plot(x_vals, y, '.', color=\"grey\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Here is the X velocity variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#X CART VELOCITY\n",
    "xd1 = np.sort(np.append(np.linspace(-1.5,1.5,4,endpoint=True),0))\n",
    "y1 = np.zeros(len(xd1)) + 0.05\n",
    "\n",
    "plt.ylim([-0.075,0.075])\n",
    "plt.plot(xd1, y1, 'o', color=\"orange\")\n",
    "plt.plot(xd_vals, y, '.', color=\"grey\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Here is the pole angle variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# POLE ANGLE\n",
    "a1 = np.sort(np.linspace(a_thres[0], a_thres[1],\n",
    "                         10, endpoint=False)[1:])\n",
    "y1 = np.zeros(len(a1)) + 0.05\n",
    "\n",
    "plt.ylim([-0.1, 0.1])\n",
    "plt.plot(a1, y1, 'o', color=\"darkcyan\")\n",
    "plt.plot(a_vals, y, '.', color=\"grey\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Here is the pole tip velocity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# POLE TIP VELOCITY\n",
    "# \n",
    "all_vals = np.sort(np.append(\n",
    "    (np.logspace(-7, 4, 6, endpoint=False, base=2)[1:],\n",
    "    -np.logspace(-7, 4, 6, endpoint=False, base=2)[1:]), 0))\n",
    "idxs = np.where(np.abs(all_vals) < 2)\n",
    "ad1 = all_vals[idxs]\n",
    "y1 = np.zeros(len(ad1)) + 0.05\n",
    "\n",
    "plt.ylim([-0.075, 0.075])\n",
    "plt.plot(ad1, y1, 'o', color=\"tomato\")\n",
    "plt.plot(ad_vals, y, '.', color=\"grey\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "x1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we will use the digitize method to determine which bucket the X coordinate would fall into. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(np.digitize(-.7, x1))\n",
    "print(np.digitize(-0.25, x1))\n",
    "print(np.digitize(0, x1))\n",
    "print(np.digitize(0.17, x1))\n",
    "print(np.digitize(5, x1))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Q TABLE\n",
    "\n",
    "\"This gives you an idea how we can group the X coordinates based on the buckets we created.\n",
    "\n",
    "Here are all the buckets in a single graph to give you an idea of our Q table.\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "yx1 = np.zeros_like(x1) + 0.25\n",
    "yx = np.zeros_like(x_vals) + 0.20\n",
    "yxd1 = np.zeros_like(xd1) + 0.10\n",
    "yxd = np.zeros_like(xd_vals) + 0.05\n",
    "ya1 = np.zeros_like(a1) - 0.05\n",
    "ya = np.zeros_like(a_vals) - 0.10\n",
    "yad1 = np.zeros_like(ad1) - 0.20\n",
    "yad = np.zeros_like(ad_vals) - 0.25\n",
    "\n",
    "plt.ylim([-0.3, 0.3])\n",
    "\n",
    "plt.plot(x1, yx1, '|')\n",
    "plt.plot(xd1, yxd1, '|')\n",
    "plt.plot(a1, ya1, '|')\n",
    "plt.plot(ad1, yad1, '|')\n",
    "\n",
    "plt.plot(x_vals, yx, '.')\n",
    "plt.plot(xd_vals, yxd, '.')\n",
    "plt.plot(a_vals, ya, '.')\n",
    "plt.plot(ad_vals, yad, '.')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, knowing how we are going to turn our continous environment into a discrete Q table we can move forward with the Q-learning algorithm. It will be very similar to the previous one. \n",
    "\n",
    "Update learning_schedule and action_selection to try and beat the scores. Either with fewer episodes or higher max values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The following 3 method can be updated to change the functionality of the Q-Learner algorithm. Update these to try and solve the environment in less episodes or reach a higher maximum value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Rate at which we set the learning rate. Typically, I keep this constant but much smarter people than myself\n",
    "#have altered this value. Either by burning it down each episode or doing a step down where they keep the\n",
    "#value constant for X episodes and then drop it.\n",
    "def learning_schedule(e, max_episode):\n",
    "    return 0.8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Action selection AKA explore/exploit. Much like the learning rate this can either burn down or step down\n",
    "def action_selection(state, Q, e, max_episodes):\n",
    "    #0-1k: 100% explore\n",
    "    #1k-2k: 80%\n",
    "    #2k-3k: 75%\n",
    "    #3k-4k: 50%\n",
    "    #4k-5k: 0%\n",
    "    if e < 1000:\n",
    "        epsilon = 0.99\n",
    "    elif e < 2000:\n",
    "        epsilon = 0.50\n",
    "    elif e < 3000:\n",
    "        epsilon = 0.25\n",
    "    elif e < 4000:\n",
    "        epsilon = 0.05\n",
    "    else:\n",
    "        epsilon = 0\n",
    "    action = np.random.randint(Q.shape[1]) if np.random.random() < epsilon else np.argmax(Q[state])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "This is a copy of what was created above for the buckets but I recreated it here to make it easier to change them in a single spot."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def build_buckets():\n",
    "    observations = []\n",
    "    for episode in range(1000): #1k episodes should give us enough data\n",
    "        observation = envCartPole.reset()\n",
    "        for step in range(100): #Do only 100 steps per episode unless we finish\n",
    "            observations.append(observation)\n",
    "            action = envCartPole.action_space.sample()\n",
    "            if episode < 25: #First 20 episodes only go left\n",
    "                action = 0\n",
    "            elif episode < 50: #Next 20 episodes only go right\n",
    "                action = 1\n",
    "            observation, reward, done, info = envCartPole.step(action)\n",
    "            if done: #Fell or hit the goal\n",
    "                break\n",
    "    envCartPole.close()\n",
    "    x_vals_loc = np.array(observations)[:,0]\n",
    "    xd_vals_loc = np.array(observations)[:,1]\n",
    "    a_vals_loc = np.array(observations)[:,2]\n",
    "    ad_vals_loc = np.array(observations)[:,3]\n",
    "    y = np.zeros_like(x_vals_loc)\n",
    "\n",
    "    x_thres_loc = ((envCartPole.env.observation_space.low/2)[0],\n",
    "           (envCartPole.env.observation_space.high/2)[0])\n",
    "    a_thres_loc = ((envCartPole.env.observation_space.low/2)[2],\n",
    "           (envCartPole.env.observation_space.high/2)[2])\n",
    "\n",
    "    x1_loc = np.linspace(np.min(x_vals_loc), np.max(x_vals_loc), 4, endpoint=False)[1:]\n",
    "    xd1_loc = np.sort(np.append(np.linspace(-1.5,1.5,4,endpoint=True),0))\n",
    "    a1_loc = np.sort(np.linspace(a_thres_loc[0], a_thres_loc[1],10, endpoint=False)[1:])\n",
    "    all_vals_loc = np.sort(np.append(\n",
    "        (np.logspace(-7, 4, 6, endpoint=False, base=2)[1:],\n",
    "         -np.logspace(-7, 4, 6, endpoint=False, base=2)[1:]), 0))\n",
    "    idxs_loc = np.where(np.abs(all_vals_loc) < 2)\n",
    "    ad1_loc = all_vals[idxs_loc]\n",
    "\n",
    "    return (x1_loc, xd1_loc, a1_loc, ad1_loc)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the method that will convert the 4 observations (X coordinate for location, the velocity of the cart, the angle of the pole, and the velocity at the tip of the pole) into a single state in the Q table. It grabs the bin for each of the 4 observations and then concatenate them into a string.\n",
    "\n",
    "For example, if the X coordinate is 0.04 and that is bin 1, x velocity is 0.02 and that is bin 2, angle is 0.02 and that is bin 1, and tip velocity is 0.04 and that is bin 6 it will get converted to Q table index 1216.\n",
    "\n",
    "To use the previous example, we knew the state space of the FrozenLake was 8 spaces so we could just use the index 1 through 8 to find the Q table location. Since we don't have that we can ensure we have enough space by putting the first observations (X coord) as the 1000s digit, second observations (x velocity) as the 100s digit, third observations (angle) as the 10s digit, and the fourth observations (tip velocity) as the 1s digit. This adds some bloat if you have less than 10 bins per state but this makes the math easy."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def observation_to_state(observation, bucket):\n",
    "    disc_state = []\n",
    "    for i in range(len(observation)):\n",
    "        disc_state.append(int(np.digitize(observation[i], bins=bucket[i])))\n",
    "    state = int(''.join(map(lambda feature: str(int(feature)), disc_state)))\n",
    "    return state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the Q-Learning algorithm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def q_learning_continuous(env, buckets):\n",
    "    gamma = 0.99\n",
    "    rewards = []    \n",
    "\n",
    "    nS = 10 * 10 * 10 * 10 #I have 4 observations so I need 4 bits\n",
    "    nA = envCartPole.action_space.n\n",
    "    Q = np.random.random((nS,nA)) #Initialize the Q table to all random numbers\n",
    "    episodes = 5000\n",
    "    for e in range(episodes):\n",
    "        observation = reset()\n",
    "        state = observation_to_state(observation, buckets)\n",
    "        total_reward = 0\n",
    "        while True:\n",
    "            envCartPole.render() #####################################################\n",
    "            action = action_selection(state, Q, e, episodes) #Grab the action\n",
    "            observation, reward, done, infor = env.step(action)\n",
    "            nstate = observation_to_state(observation,buckets)\n",
    "            total_reward += reward\n",
    "\n",
    "            alpha = learning_schedule(e,episodes)\n",
    "\n",
    "            #Q Function Update\n",
    "            #(not done) keeps the terminal state as 0\n",
    "            Q[state][action] += alpha * (reward + gamma * Q[nstate].max() * (not done) - Q[state][action])\n",
    "            state = nstate\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_reward) #Keep track of the total rewards per episode\n",
    "                break\n",
    "    return Q, rewards\n",
    "\n",
    "#envCartPole.close() ##################################################"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Here is the calling code"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "buckets = build_buckets() #Create the buckets\n",
    "Q, rewards = q_learning_continuous(envCartPole, buckets)\n",
    "envCartPole.close()\n",
    "print(Q) #Here is the Q table. It will only print part of it but it shows the 2 actions per state"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we wil graph the results. Hopefully, you will see the rolling averge break the red \"goal\" line. One thing to note is look how the rolling average changes based on your epsilon and alpha changes."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}