{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### FrozenLake-v1\n",
    "\"The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\"\n",
    "https://gym.openai.com/envs/FrozenLake-v0/ \n",
    "\n",
    "The below is Frozen Lake set to slippery. The agent has a 1/3 chance of slipping and not moving in the direction intended.\n",
    "\n",
    "#### SUMMARY\n",
    "Environment: The whole grid world including all squares e.g. 4x4 = 16 squares\n",
    "State: One square\n",
    "Agent: Can occupy one state at a time, can perform actions (up, down, left, right). As the agent travels and explores the environment it will work out th ebest policy.\n",
    "Reward: +1 when finding the fisbee goal, -1 when falling in a hole.Can implement a negative reward eg -0.1 per step i.e. higher reward if reached faster.\n",
    "Policy: Map & information about what action to take in a particular state. The best policy is to thake the best action in a particular state. Find a policy that allows agent to gain maxium reward.\n",
    "\n",
    "###### Q-TABLE (reward table)\n",
    "As the agent expolores, it records the best action to take for each state (square) in the Q-Table \n",
    "e.g. If you start in state C1 and the reward is at A1, it is best to move up when in square C1. From B1 it is then best to move up and get the reward at A1.\n",
    "\n",
    "###### Goal: The way to choose the best action fr every state in the environment.\n",
    "- Gym presets the reward ie cannot change reward to solve these problems. \n",
    "- You can change how the q-table is populated. You can change how the Q-table is calculated on each step. e.g. Keep track of the reward for each step and long term."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install gym"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "gc.disable() #Disable automatic garbage collection."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "action_space_size = env.action_space.n\n",
    "state_space_size = env.observation_space.n\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_episodes = 20000\n",
    "max_steps_per_episode = 100\n",
    "\n",
    "learning_rate = 0.03 #0.03 #Between 0 and 1. How quickly the agent abandons the previous value in the Q table for the new value. 0: Agent learns nothing and only uses prior knowledge. 1: Agent considers only the most recent information.\n",
    "discount_rate = 0.97 #0.97\n",
    "\n",
    "exploration_rate = 1\n",
    "max_exploration_rate = 1    #1: Guaranteed that agent starts the game by 100% exploring the environment\n",
    "min_exploration_rate = 0    #0: Agent does not explore at all. Agent only exploits (chooses actions to get max points)\n",
    "exploration_decay_rate = 0.001"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rewards_all_episodes = []\n",
    "\n",
    "# Q-Learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    \n",
    "    # random_map = generate_random_map(size=4, p=0.8)\n",
    "    # env = gym.make(\"FrozenLake-v0\", desc=random_map)\n",
    "    env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    rewards_current_episode = 0\n",
    "\n",
    "    for step in range (max_steps_per_episode):\n",
    "\n",
    "        # Exploration / Exploitation trade-off\n",
    "        exploration_rate_threshold = random.uniform(0, 1)\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "\n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate + \\\n",
    "        (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "# Calculate reward across all episodes\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes / 1000)\n",
    "count = 1000\n",
    "count_plot = [] #for plotting\n",
    "r_plot = [] #for plotting\n",
    "print(\"*** AVG reward per 1000 episodes***\\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \": \", str(sum(r/1000)))\n",
    "    count_plot.append(count) #for plotting. \n",
    "    r_plot.append(sum(r/1000)) #for plotting\n",
    "    count += 1000\n",
    "\n",
    "\n",
    "# Print Q table\n",
    "print(\"\\n\\n *** Q-table ***\\n\")\n",
    "print(q_table)\n",
    "#note that the q-table cannot be fully completed. If the agent falls into a hole it dies and has no idea what happened so that slot is zero.\n",
    "\n",
    "#Plot results \n",
    "%matplotlib inline\n",
    "#Name x-axis, y-axis and whole graph\n",
    "plt.xlabel(\"episodes\")\n",
    "plt.ylabel(\"average rewards\")\n",
    "plt.title(\"FROZEN LAKE: AVG reward per 1000 episodes\")\n",
    "# Plotting all the graphs\n",
    "plt.plot(count_plot, r_plot, color=\"darkviolet\", label = \"Push\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#Load the display window\n",
    "plt.show\n",
    "#set y-axis limit\n",
    "plt.ylim([0, 1])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# random_map = generate_random_map(size=4, p=0.8)\n",
    "# env = gym.make(\"FrozenLake-v0\", desc=random_map)\n",
    "env = gym.make('FrozenLake-v1', is_slippery=True)\n",
    "for episode in range(10): #number of episodes played through - up to 20,000\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"*****EPISODE \", episode+1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(0.5) #changed from 1\n",
    "\n",
    "    for step in range(max_steps_per_episode):        \n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        time.sleep(0.1) #changed from 0.3\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])        \n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(0.5) #changed from 3\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(0.5) #changed from 3\n",
    "            clear_output(wait=True)\n",
    "            break\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}